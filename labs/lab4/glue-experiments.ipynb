{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5301047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL=\"prajjwal1/bert-small\"\n",
    "TASKS=[\"cola\", \"qnli\", \"qqp\", \"mnli\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7c325",
   "metadata": {},
   "source": [
    "##  Load Pretrained Model \n",
    "\n",
    "Code for loading a pretrained BERT checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d6eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2267f",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Loads the dataset for GLUE task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b16dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/pfernand/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa15eef714541ffa17fcada2eb4600b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/pfernand/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7c1f3d71094adcb67439f95e6eb73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/pfernand/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3b2d793f804382aba978d76b29508c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/pfernand/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fddaf49547247479f7355a490b30262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "datasets = {}\n",
    "for task in TASKS:\n",
    "    datasets[task] = load_dataset('glue', task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def tokenize_function(examples, task, max_seq_length=512):\n",
    "    # Tokenize the texts\n",
    "    sentence1_key, sentence2_key = task_to_keys[task]\n",
    "    args = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    return tokenizer(*args, max_length=max_seq_length, truncation=True)\n",
    "\n",
    "tokenized_datasets = {}\n",
    "for task in TASKS:\n",
    "    tokenized_datasets[task] = datasets[task].map(lambda e: tokenize_function(e, task=task), batched=True)\n",
    "    \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46ca4d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363846\n"
     ]
    }
   ],
   "source": [
    "for task in TASKS:\n",
    "    print(len(datasets[task][\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ff3d6",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1030f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metrics = {task: load_metric(\"glue\", task) for task in TASKS}\n",
    "\n",
    "def compute_metrics(p, task):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    result = metrics[task].compute(predictions=preds, references=p.label_ids)\n",
    "    if len(result) > 1:\n",
    "        result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fdd63ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/pfernand/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/pfernand/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence.\n",
      "***** Running training *****\n",
      "  Num examples = 8551\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 402\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 31.75 GiB total capacity; 30.36 GiB already allocated; 9.75 MiB free; 30.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10535/2116224804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinetuned_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m                 if (\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1530\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m         )\n\u001b[0;32m--> 995\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    996\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 )\n\u001b[1;32m    581\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    583\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     ):\n\u001b[0;32m--> 401\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/tir4/users/pfernand/miniconda3/envs/11-767/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 31.75 GiB total capacity; 30.36 GiB already allocated; 9.75 MiB free; 30.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "%%capture output\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "for task in TASKS:\n",
    "    num_labels=len(datasets[task][\"train\"].features[\"label\"].names)\n",
    "    finetuned_path = f\"{PRETRAINED_MODEL.split('/')[-1]}-ft-{task}\"\n",
    "    model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=num_labels)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = finetuned_path, \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        per_device_train_batch_size=64,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        train_dataset=tokenized_datasets[task][\"train\"],\n",
    "        eval_dataset=tokenized_datasets[task][\"validation_matched\" if task == \"mnli\" else \"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, task=task)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained(finetuned_path)\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5be4ca7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 2, 'idx': 100, 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'hypothesis': 'The exhibition only displays cars from the 2000s.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'premise': 'It displays all kinds of vehicles, from the coach that carried Napoleon to and from Moscow in 1812 to a splendid 4-horsepower Renault car from 1904 and other turn-of-the-century classics.', 'input_ids': [101, 2009, 8834, 2035, 7957, 1997, 4683, 1010, 2013, 1996, 2873, 2008, 3344, 8891, 2000, 1998, 2013, 4924, 1999, 9842, 2000, 1037, 21459, 1018, 1011, 15149, 14605, 2482, 2013, 5692, 1998, 2060, 2735, 1011, 1997, 1011, 1996, 1011, 2301, 10002, 1012, 102, 1996, 4538, 2069, 8834, 3765, 2013, 1996, 8876, 1012, 102]}\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb139b7",
   "metadata": {},
   "source": [
    "## Evaluating on Squad\n",
    "\n",
    "Code for running evaluation on finetuned model on squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da13bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "for task in TASKS:\n",
    "    finetuned_path = f\"{PRETRAINED_MODEL.split('/')[-1]}-ft-{task}\"\n",
    "    model = BertForSequenceClassification.from_pretrained(finetuned_path)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(PRETRAINED_MODEL)\n",
    "    val_key = \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir = finetuned_path,\n",
    "        do_train = False,\n",
    "        do_predict = True,\n",
    "        dataloader_drop_last = False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        eval_args,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, task=task)\n",
    "    )\n",
    "    start = timer()\n",
    "    result = trainer.evaluate(eval_dataset=tokenized_datasets[task][val_key])\n",
    "    end = timer()\n",
    "    delta = end-start\n",
    "    print(task)\n",
    "    print(result)\n",
    "    print(f\"time per sample = {delta/len(tokenized_datasets[task][val_key])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6678a88",
   "metadata": {},
   "source": [
    "## Quantitize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3a0806d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file bert-small-ft-cola/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert-small-ft-cola/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at bert-small-ft-cola.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Configuration saved in bert-small-ft-cola-qt/config.json\n",
      "Model weights saved in bert-small-ft-cola-qt/pytorch_model.bin\n",
      "loading configuration file bert-small-ft-qnli/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert-small-ft-qnli/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at bert-small-ft-qnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Configuration saved in bert-small-ft-qnli-qt/config.json\n",
      "Model weights saved in bert-small-ft-qnli-qt/pytorch_model.bin\n",
      "loading configuration file bert-small-ft-qqp/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert-small-ft-qqp/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at bert-small-ft-qqp.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Configuration saved in bert-small-ft-qqp-qt/config.json\n",
      "Model weights saved in bert-small-ft-qqp-qt/pytorch_model.bin\n",
      "loading configuration file bert-small-ft-mnli/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert-small-ft-mnli/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at bert-small-ft-mnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Configuration saved in bert-small-ft-mnli-qt/config.json\n",
      "Model weights saved in bert-small-ft-mnli-qt/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "for task in TASKS:\n",
    "    finetuned_path = f\"{PRETRAINED_MODEL.split('/')[-1]}-ft-{task}\"\n",
    "    model = BertForSequenceClassification.from_pretrained(finetuned_path)\n",
    "    qt_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "    qt_model.save_pretrained(f\"{finetuned_path}-qt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e6e9248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/vocab.txt from cache at /home/pfernand/.cache/huggingface/transformers/68be80309844e53b628e9d479926a991d0adf337752bb941f0188887240313b8.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /home/pfernand/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /home/pfernand/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file bert-small-ft-cola/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert-small-ft-cola/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at bert-small-ft-cola.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='131' max='131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [131/131 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         4.62%     113.382ms        99.83%        2.449s        2.449s       0.000us         0.00%      63.487ms      63.487ms     109.73 Mb      -4.30 Kb     110.12 Mb      -1.98 Mb             1  \n",
      "                                            aten::copy_         0.58%      14.203ms        84.86%        2.081s      13.258ms      62.858ms        99.01%      62.858ms     400.369us           0 b           0 b           0 b           0 b           157  \n",
      "                                               aten::to         0.07%       1.742ms        84.59%        2.075s      13.300ms       0.000us         0.00%      62.841ms     402.827us           0 b           0 b     110.13 Mb           0 b           156  \n",
      "                                         aten::_to_copy         0.08%       1.894ms        84.52%        2.073s      26.240ms       0.000us         0.00%      62.841ms     795.456us           0 b           0 b     110.13 Mb           0 b            79  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      62.837ms        98.98%      62.837ms     805.603us           0 b           0 b           0 b           0 b            78  \n",
      "                                           aten::linear         0.01%     244.000us         0.17%       4.212ms     162.000us       0.000us         0.00%     471.000us      18.115us           0 b           0 b     866.50 Kb           0 b            26  \n",
      "                                           aten::matmul         0.02%     530.000us         0.11%       2.716ms      84.875us       0.000us         0.00%     405.000us      12.656us           0 b           0 b     978.00 Kb           0 b            32  \n",
      "                                               aten::mm         0.03%     828.000us         0.04%       1.095ms      45.625us     381.000us         0.60%     381.000us      15.875us           0 b           0 b     864.00 Kb     864.00 Kb            24  \n",
      "void gemmSN_TN_kernel_64addr<float, 128, 16, 2, 4, 6...         0.00%       0.000us         0.00%       0.000us       0.000us     312.000us         0.49%     312.000us      13.000us           0 b           0 b           0 b           0 b            24  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      85.000us         0.13%      85.000us       3.036us           0 b           0 b           0 b           0 b            28  \n",
      "                                             aten::add_         0.01%     291.000us         0.02%     448.000us      17.920us      75.000us         0.12%      75.000us       3.000us           0 b           0 b           0 b           0 b            25  \n",
      "                         volta_sgemm_64x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us      68.000us         0.11%      68.000us      17.000us           0 b           0 b           0 b           0 b             4  \n",
      "                                       aten::layer_norm         0.00%      83.000us         0.03%     807.000us      89.667us       0.000us         0.00%      54.000us       6.000us           0 b           0 b     225.00 Kb           0 b             9  \n",
      "                                aten::native_layer_norm         0.01%     325.000us         0.03%     724.000us      80.444us      54.000us         0.09%      54.000us       6.000us           0 b           0 b     225.00 Kb           0 b             9  \n",
      "void at::native::(anonymous namespace)::RowwiseMomen...         0.00%       0.000us         0.00%       0.000us       0.000us      40.000us         0.06%      40.000us       4.444us           0 b           0 b           0 b           0 b             9  \n",
      "                                        aten::embedding         0.00%      57.000us         0.01%     249.000us      83.000us       0.000us         0.00%      25.000us       8.333us           0 b           0 b      72.00 Kb           0 b             3  \n",
      "                                     aten::index_select         0.00%      67.000us         0.01%     148.000us      49.333us      25.000us         0.04%      25.000us       8.333us           0 b           0 b      72.00 Kb           0 b             3  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us      25.000us         0.04%      25.000us       8.333us           0 b           0 b           0 b           0 b             3  \n",
      "                                              aten::bmm         0.01%     178.000us         0.01%     352.000us      44.000us      24.000us         0.04%      24.000us       3.000us           0 b           0 b     114.00 Kb           0 b             8  \n",
      "                                              aten::add         0.01%     244.000us         0.01%     326.000us      25.077us      23.000us         0.04%      23.000us       1.769us           0 b           0 b     234.00 Kb     234.00 Kb            13  \n",
      "                                       aten::contiguous         0.00%      16.000us         0.01%     205.000us      51.250us       0.000us         0.00%      17.000us       4.250us           0 b           0 b      96.00 Kb           0 b             4  \n",
      "                                            aten::clone         0.00%      52.000us         0.01%     189.000us      47.250us       0.000us         0.00%      17.000us       4.250us           0 b           0 b      96.00 Kb           0 b             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      17.000us         0.03%      17.000us       4.250us           0 b           0 b           0 b           0 b             4  \n",
      "                                            aten::addmm         0.00%      84.000us         0.01%     173.000us      86.500us      16.000us         0.03%      16.000us       8.000us           0 b           0 b       2.50 Kb       2.50 Kb             2  \n",
      "void at::native::(anonymous namespace)::LayerNormFor...         0.00%       0.000us         0.00%       0.000us       0.000us      14.000us         0.02%      14.000us       1.556us           0 b           0 b           0 b           0 b             9  \n",
      "void gemmSN_NN_kernel<float, 256, 4, 2, 8, 6, 4, fal...         0.00%       0.000us         0.00%       0.000us       0.000us      14.000us         0.02%      14.000us       3.500us           0 b           0 b           0 b           0 b             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      13.000us         0.02%      13.000us       1.300us           0 b           0 b           0 b           0 b            10  \n",
      "void splitKreduce_kernel<float, float, float, float>...         0.00%       0.000us         0.00%       0.000us       0.000us      11.000us         0.02%      11.000us       2.750us           0 b           0 b           0 b           0 b             4  \n",
      "                                             aten::gelu         0.00%      90.000us         0.00%     119.000us      29.750us      10.000us         0.02%      10.000us       2.500us           0 b           0 b     384.00 Kb     384.00 Kb             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.000us         0.02%      10.000us       2.500us           0 b           0 b           0 b           0 b             4  \n",
      "                                          aten::softmax         0.00%      24.000us         0.01%     137.000us      34.250us       0.000us         0.00%       9.000us       2.250us           0 b           0 b      18.00 Kb           0 b             4  \n",
      "                                         aten::_softmax         0.00%      83.000us         0.00%     113.000us      28.250us       9.000us         0.01%       9.000us       2.250us           0 b           0 b      18.00 Kb      18.00 Kb             4  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us       9.000us         0.01%       9.000us       2.250us           0 b           0 b           0 b           0 b             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.000us         0.01%       8.000us       1.600us           0 b           0 b           0 b           0 b             5  \n",
      "                                              aten::div         0.00%      91.000us         0.00%     120.000us      30.000us       7.000us         0.01%       7.000us       1.750us           0 b           0 b      18.00 Kb      18.00 Kb             4  \n",
      "void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us         0.01%       6.000us       6.000us           0 b           0 b           0 b           0 b             1  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         0.01%       4.000us       4.000us           0 b           0 b           0 b           0 b             1  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         0.01%       4.000us       4.000us           0 b           0 b           0 b           0 b             1  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         0.00%       3.000us       1.500us           0 b           0 b           0 b           0 b             2  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         0.00%       3.000us       3.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                             aten::rsub         0.00%      45.000us         0.00%     113.000us     113.000us       0.000us         0.00%       2.000us       2.000us           0 b           0 b         512 b           0 b             1  \n",
      "                                              aten::sub         0.00%      53.000us         0.00%      68.000us      68.000us       2.000us         0.00%       2.000us       2.000us           0 b           0 b         512 b         512 b             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         0.00%       2.000us       2.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                             aten::tanh         0.00%      24.000us         0.00%      33.000us      33.000us       2.000us         0.00%       2.000us       2.000us           0 b           0 b       2.00 Kb       2.00 Kb             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         0.00%       2.000us       2.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                              aten::mul         0.00%      23.000us         0.00%      32.000us      32.000us       1.000us         0.00%       1.000us       1.000us           0 b           0 b         512 b         512 b             1  \n",
      "                                            aten::zeros         0.17%       4.204ms         0.17%       4.255ms       2.127ms       0.000us         0.00%       0.000us       0.000us       4.00 Kb           0 b           0 b           0 b             2  \n",
      "                                            aten::empty         0.16%       4.047ms         0.16%       4.047ms      20.648us       0.000us         0.00%       0.000us       0.000us     109.73 Mb     109.73 Mb     435.00 Kb     435.00 Kb           196  \n",
      "                                            aten::zero_         0.00%      24.000us         0.00%      24.000us       2.182us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            11  \n",
      "                                          aten::detach_         0.02%     387.000us         0.02%     509.000us       6.610us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            77  \n",
      "                                                detach_         0.00%     122.000us         0.00%     122.000us       1.584us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            77  \n",
      "                                             aten::set_         0.01%     271.000us         0.01%     271.000us       3.662us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            74  \n",
      "                                           aten::detach         0.01%     358.000us         0.03%     725.000us       4.932us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           147  \n",
      "                                                 detach         0.01%     367.000us         0.01%     367.000us       2.497us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           147  \n",
      "                                          aten::normal_         5.99%     146.881ms         5.99%     146.881ms      48.960ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             3  \n",
      "                                           aten::select         0.00%      24.000us         0.00%      28.000us      14.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             2  \n",
      "                                       aten::as_strided         0.00%     112.000us         0.00%     112.000us       1.455us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            77  \n",
      "                                            aten::fill_         0.00%      56.000us         0.00%      56.000us       5.600us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            10  \n",
      "                                           aten::arange         0.00%      30.000us         0.00%      71.000us      35.500us       0.000us         0.00%       0.000us       0.000us       8.00 Kb           0 b           0 b           0 b             2  \n",
      "                                          aten::resize_         0.00%      53.000us         0.00%      53.000us      13.250us       0.000us         0.00%       0.000us       0.000us       4.00 Kb       4.00 Kb      72.00 Kb      72.00 Kb             4  \n",
      "                                           aten::expand         0.01%     166.000us         0.01%     188.000us       9.895us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            19  \n",
      "                                         aten::uniform_         3.57%      87.481ms         3.57%      87.481ms       1.682ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            52  \n",
      "                                    aten::empty_strided         0.11%       2.623ms         0.11%       2.623ms      33.203us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     110.13 Mb     110.13 Mb            79  \n",
      "                                        cudaMemcpyAsync         3.38%      83.019ms         3.38%      83.019ms       1.038ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            80  \n",
      "                                  cudaStreamSynchronize         0.12%       2.904ms         0.12%       2.904ms      37.231us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            78  \n",
      "                aten::_has_compatible_shallow_copy_type         0.01%     145.000us         0.01%     145.000us       0.993us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           146  \n",
      "                                        aten::unsqueeze         0.00%      62.000us         0.00%      73.000us      14.600us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                                            aten::slice         0.00%      63.000us         0.00%      72.000us      14.400us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                                       cudaLaunchKernel        80.81%        1.982s        80.81%        1.982s      16.798ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           118  \n",
      "                                          aten::reshape         0.00%     116.000us         0.01%     196.000us      10.316us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            19  \n",
      "                                   aten::_reshape_alias         0.00%      80.000us         0.00%      80.000us       4.211us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            19  \n",
      "                                             aten::view         0.01%     281.000us         0.01%     281.000us       3.022us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            93  \n",
      "                                          aten::dropout         0.00%      10.000us         0.00%      10.000us       0.714us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            14  \n",
      "                                                aten::t         0.01%     250.000us         0.06%       1.579ms      60.731us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            26  \n",
      "                                        aten::transpose         0.05%       1.320ms         0.06%       1.366ms      45.533us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            30  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.00%      26.000us         0.00%      26.000us       0.520us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            50  \n",
      "                                     aten::_unsafe_view         0.01%     267.000us         0.01%     325.000us      10.156us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            32  \n",
      "                                          aten::permute         0.01%     141.000us         0.01%     161.000us      10.062us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            16  \n",
      "                                       aten::empty_like         0.00%      22.000us         0.00%      61.000us      15.250us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      96.00 Kb           0 b             4  \n",
      "                                         cudaEventQuery         0.00%      14.000us         0.00%      14.000us       2.800us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                                        cudaEventRecord         0.00%       6.000us         0.00%       6.000us       1.200us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                                  cudaDeviceSynchronize         0.00%       7.000us         0.00%       7.000us       7.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.453s\n",
      "Self CUDA time total: 63.487ms\n",
      "\n",
      "time per sample = 0.0018412590987644626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/vocab.txt from cache at /home/pfernand/.cache/huggingface/transformers/68be80309844e53b628e9d479926a991d0adf337752bb941f0188887240313b8.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /home/pfernand/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /home/pfernand/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file bert-small-ft-qnli/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert-small-ft-qnli/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at bert-small-ft-qnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx, question.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5463\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='683' max='683' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [683/683 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         4.61%     100.623ms        99.79%        2.178s        2.178s       0.000us         0.00%      26.541ms      26.541ms     109.73 Mb      -4.75 Kb     110.12 Mb      -5.40 Mb             1  \n",
      "                                            aten::copy_         0.56%      12.170ms        83.55%        1.824s      11.616ms      25.946ms        97.76%      25.946ms     165.261us           0 b           0 b           0 b           0 b           157  \n",
      "                                               aten::to         0.05%       1.101ms        83.23%        1.817s      11.645ms       0.000us         0.00%      25.925ms     166.186us           0 b           0 b     110.13 Mb           0 b           156  \n",
      "                                         aten::_to_copy         0.06%       1.317ms        83.18%        1.816s      22.982ms       0.000us         0.00%      25.925ms     328.165us           0 b           0 b     110.13 Mb           0 b            79  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      25.921ms        97.66%      25.921ms     332.321us           0 b           0 b           0 b           0 b            78  \n",
      "                                           aten::linear         0.01%     221.000us         0.19%       4.056ms     156.000us       0.000us         0.00%     411.000us      15.808us           0 b           0 b       2.18 Mb           0 b            26  \n",
      "                                           aten::matmul         0.02%     495.000us         0.13%       2.910ms      90.938us       0.000us         0.00%     381.000us      11.906us           0 b           0 b       2.54 Mb           0 b            32  \n",
      "                                               aten::mm         0.04%     905.000us         0.06%       1.386ms      57.750us     321.000us         1.21%     321.000us      13.375us           0 b           0 b       2.18 Mb       2.18 Mb            24  \n",
      "                         volta_sgemm_32x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us     200.000us         0.75%     200.000us      10.000us           0 b           0 b           0 b           0 b            20  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      86.000us         0.32%      86.000us       3.071us           0 b           0 b           0 b           0 b            28  \n",
      "                                             aten::add_         0.01%     292.000us         0.02%     467.000us      18.680us      77.000us         0.29%      77.000us       3.080us           0 b           0 b           0 b           0 b            25  \n",
      "                         volta_sgemm_64x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us      65.000us         0.24%      65.000us      16.250us           0 b           0 b           0 b           0 b             4  \n",
      "                                              aten::bmm         0.01%     233.000us         0.02%     386.000us      48.250us      60.000us         0.23%      60.000us       7.500us           0 b           0 b     370.00 Kb           0 b             8  \n",
      "                                       aten::layer_norm         0.00%      47.000us         0.03%     708.000us      78.667us       0.000us         0.00%      59.000us       6.556us           0 b           0 b     567.00 Kb           0 b             9  \n",
      "                                aten::native_layer_norm         0.01%     293.000us         0.03%     661.000us      73.444us      59.000us         0.22%      59.000us       6.556us           0 b           0 b     567.00 Kb           0 b             9  \n",
      "void splitKreduce_kernel<float, float, float, float>...         0.00%       0.000us         0.00%       0.000us       0.000us      56.000us         0.21%      56.000us       2.333us           0 b           0 b           0 b           0 b            24  \n",
      "void at::native::(anonymous namespace)::RowwiseMomen...         0.00%       0.000us         0.00%       0.000us       0.000us      38.000us         0.14%      38.000us       4.222us           0 b           0 b           0 b           0 b             9  \n",
      "                                  volta_sgemm_128x32_tn         0.00%       0.000us         0.00%       0.000us       0.000us      33.000us         0.12%      33.000us       8.250us           0 b           0 b           0 b           0 b             4  \n",
      "                                  volta_sgemm_128x32_nn         0.00%       0.000us         0.00%       0.000us       0.000us      27.000us         0.10%      27.000us       6.750us           0 b           0 b           0 b           0 b             4  \n",
      "                                              aten::add         0.01%     256.000us         0.02%     350.000us      26.923us      21.000us         0.08%      21.000us       1.615us           0 b           0 b     680.00 Kb     680.00 Kb            13  \n",
      "void at::native::(anonymous namespace)::LayerNormFor...         0.00%       0.000us         0.00%       0.000us       0.000us      21.000us         0.08%      21.000us       2.333us           0 b           0 b           0 b           0 b             9  \n",
      "                                       aten::contiguous         0.00%      25.000us         0.01%     210.000us      52.500us       0.000us         0.00%      21.000us       5.250us           0 b           0 b     248.00 Kb           0 b             4  \n",
      "                                            aten::clone         0.00%      46.000us         0.01%     185.000us      46.250us       0.000us         0.00%      21.000us       5.250us           0 b           0 b     248.00 Kb           0 b             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      21.000us         0.08%      21.000us       5.250us           0 b           0 b           0 b           0 b             4  \n",
      "                                            aten::addmm         0.00%      80.000us         0.01%     153.000us      76.500us      14.000us         0.05%      14.000us       7.000us           0 b           0 b       2.50 Kb       2.50 Kb             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      12.000us         0.05%      12.000us       1.200us           0 b           0 b           0 b           0 b            10  \n",
      "                                             aten::gelu         0.00%      81.000us         0.01%     125.000us      31.250us      12.000us         0.05%      12.000us       3.000us           0 b           0 b     992.00 Kb     992.00 Kb             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      12.000us         0.05%      12.000us       3.000us           0 b           0 b           0 b           0 b             4  \n",
      "                                        aten::embedding         0.00%      52.000us         0.01%     200.000us      66.667us       0.000us         0.00%      10.000us       3.333us           0 b           0 b     186.00 Kb           0 b             3  \n",
      "                                     aten::index_select         0.00%      53.000us         0.01%     118.000us      39.333us      10.000us         0.04%      10.000us       3.333us           0 b           0 b     186.00 Kb           0 b             3  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us      10.000us         0.04%      10.000us       3.333us           0 b           0 b           0 b           0 b             3  \n",
      "                                          aten::softmax         0.00%      21.000us         0.01%     129.000us      32.250us       0.000us         0.00%      10.000us       2.500us           0 b           0 b     122.00 Kb           0 b             4  \n",
      "                                         aten::_softmax         0.00%      79.000us         0.00%     108.000us      27.000us      10.000us         0.04%      10.000us       2.500us           0 b           0 b     122.00 Kb     122.00 Kb             4  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us      10.000us         0.04%      10.000us       2.500us           0 b           0 b           0 b           0 b             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.000us         0.03%       7.000us       1.400us           0 b           0 b           0 b           0 b             5  \n",
      "                                              aten::div         0.00%      97.000us         0.01%     127.000us      31.750us       6.000us         0.02%       6.000us       1.500us           0 b           0 b     122.00 Kb     122.00 Kb             4  \n",
      "void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us         0.02%       6.000us       6.000us           0 b           0 b           0 b           0 b             1  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         0.02%       4.000us       4.000us           0 b           0 b           0 b           0 b             1  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         0.01%       3.000us       1.500us           0 b           0 b           0 b           0 b             2  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         0.01%       3.000us       3.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                             aten::rsub         0.00%      31.000us         0.00%      95.000us      95.000us       0.000us         0.00%       2.000us       2.000us           0 b           0 b         512 b           0 b             1  \n",
      "                                              aten::sub         0.00%      49.000us         0.00%      64.000us      64.000us       2.000us         0.01%       2.000us       2.000us           0 b           0 b         512 b         512 b             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         0.01%       2.000us       2.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                             aten::tanh         0.00%      23.000us         0.00%      30.000us      30.000us       2.000us         0.01%       2.000us       2.000us           0 b           0 b       2.00 Kb       2.00 Kb             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         0.01%       2.000us       2.000us           0 b           0 b           0 b           0 b             1  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         0.01%       2.000us       2.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                              aten::mul         0.00%      18.000us         0.00%      26.000us      26.000us       1.000us         0.00%       1.000us       1.000us           0 b           0 b         512 b         512 b             1  \n",
      "                                            aten::zeros         0.09%       2.032ms         0.21%       4.519ms       2.260ms       0.000us         0.00%       0.000us       0.000us       4.00 Kb           0 b           0 b           0 b             2  \n",
      "                                            aten::empty         0.30%       6.640ms         0.30%       6.640ms      33.878us       0.000us         0.00%       0.000us       0.000us     109.73 Mb     109.73 Mb       1.16 Mb       1.16 Mb           196  \n",
      "                                            aten::zero_         0.00%      23.000us         0.00%      23.000us       2.091us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            11  \n",
      "                                          aten::detach_         0.02%     465.000us         0.03%     599.000us       7.779us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            77  \n",
      "                                                detach_         0.01%     134.000us         0.01%     134.000us       1.740us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            77  \n",
      "                                             aten::set_         0.01%     300.000us         0.01%     300.000us       4.054us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            74  \n",
      "                                           aten::detach         0.02%     489.000us         0.04%     848.000us       5.769us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           147  \n",
      "                                                 detach         0.02%     359.000us         0.02%     359.000us       2.442us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           147  \n",
      "                                          aten::normal_         6.93%     151.177ms         6.93%     151.177ms      50.392ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             3  \n",
      "                                           aten::select         0.00%      27.000us         0.00%      32.000us      16.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             2  \n",
      "                                       aten::as_strided         0.01%     115.000us         0.01%     115.000us       1.494us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            77  \n",
      "                                            aten::fill_         0.00%      57.000us         0.00%      57.000us       5.700us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            10  \n",
      "                                           aten::arange         0.00%      29.000us         0.00%      70.000us      35.000us       0.000us         0.00%       0.000us       0.000us       8.00 Kb           0 b           0 b           0 b             2  \n",
      "                                          aten::resize_         0.00%      45.000us         0.00%      45.000us      11.250us       0.000us         0.00%       0.000us       0.000us       4.00 Kb       4.00 Kb     186.00 Kb     186.00 Kb             4  \n",
      "                                           aten::expand         0.01%     150.000us         0.01%     173.000us       9.105us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            19  \n",
      "                                         aten::uniform_         3.91%      85.371ms         3.91%      85.371ms       1.642ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            52  \n",
      "                                    aten::empty_strided         0.08%       1.811ms         0.08%       1.811ms      22.924us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     110.13 Mb     110.13 Mb            79  \n",
      "                                        cudaMemcpyAsync         1.48%      32.274ms         1.48%      32.274ms     403.425us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            80  \n",
      "                                  cudaStreamSynchronize         0.13%       2.881ms         0.13%       2.881ms      36.936us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            78  \n",
      "                aten::_has_compatible_shallow_copy_type         0.01%     121.000us         0.01%     121.000us       0.829us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           146  \n",
      "                                        aten::unsqueeze         0.00%      48.000us         0.00%      58.000us      11.600us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                                            aten::slice         0.00%      52.000us         0.00%      60.000us      12.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                                       cudaLaunchKernel        81.43%        1.777s        81.43%        1.777s      12.880ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           138  \n",
      "                                          aten::reshape         0.00%      97.000us         0.01%     164.000us       8.632us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            19  \n",
      "                                   aten::_reshape_alias         0.00%      67.000us         0.00%      67.000us       3.526us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            19  \n",
      "                                             aten::view         0.01%     231.000us         0.01%     231.000us       2.484us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            93  \n",
      "                                          aten::dropout         0.00%      10.000us         0.00%      10.000us       0.714us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            14  \n",
      "                                                aten::t         0.01%     214.000us         0.06%       1.226ms      47.154us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            26  \n",
      "                                        aten::transpose         0.05%     999.000us         0.05%       1.046ms      34.867us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            30  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.00%      27.000us         0.00%      27.000us       0.365us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            74  \n",
      "                                         cudaEventQuery         0.00%      63.000us         0.00%      63.000us       2.520us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            25  \n",
      "                                        cudaEventRecord         0.00%      26.000us         0.00%      26.000us       1.040us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            25  \n",
      "                                     aten::_unsafe_view         0.01%     219.000us         0.01%     272.000us       8.500us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            32  \n",
      "                                          aten::permute         0.01%     128.000us         0.01%     150.000us       9.375us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            16  \n",
      "                                       aten::empty_like         0.00%      24.000us         0.00%      64.000us      16.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     248.00 Kb           0 b             4  \n",
      "                                  cudaDeviceSynchronize         0.00%       7.000us         0.00%       7.000us       7.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.183s\n",
      "Self CUDA time total: 26.541ms\n",
      "\n",
      "time per sample = 0.0008929177059013067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/vocab.txt from cache at /home/pfernand/.cache/huggingface/transformers/68be80309844e53b628e9d479926a991d0adf337752bb941f0188887240313b8.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /home/pfernand/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /home/pfernand/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file bert-small-ft-qqp/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert-small-ft-qqp/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at bert-small-ft-qqp.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question1, idx, question2.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 40430\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5054' max='5054' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5054/5054 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         6.35%     149.698ms        99.55%        2.346s        2.346s       0.000us         0.00%      43.683ms      43.683ms     109.73 Mb      -4.44 Kb     109.74 Mb      -3.03 Mb             1  \n",
      "                                            aten::copy_         1.25%      29.573ms        83.15%        1.959s      12.481ms      43.110ms        98.69%      43.110ms     274.586us           0 b           0 b           0 b           0 b           157  \n",
      "                                               aten::to         0.13%       3.050ms        82.28%        1.939s      12.429ms       0.000us         0.00%      43.091ms     276.224us           0 b           0 b     109.74 Mb           0 b           156  \n",
      "                                         aten::_to_copy         0.06%       1.402ms        82.16%        1.936s      24.506ms       0.000us         0.00%      43.091ms     545.456us           0 b           0 b     109.74 Mb           0 b            79  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      43.087ms        98.64%      43.087ms     552.397us           0 b           0 b           0 b           0 b            78  \n",
      "                                           aten::linear         0.01%     207.000us         0.15%       3.605ms     138.654us       0.000us         0.00%     392.000us      15.077us           0 b           0 b       1.27 Mb           0 b            26  \n",
      "                                           aten::matmul         0.02%     459.000us         0.11%       2.632ms      82.250us       0.000us         0.00%     361.000us      11.281us           0 b           0 b       1.45 Mb           0 b            32  \n",
      "                                               aten::mm         0.03%     818.000us         0.05%       1.243ms      51.792us     303.000us         0.69%     303.000us      12.625us           0 b           0 b       1.27 Mb       1.27 Mb            24  \n",
      "                         volta_sgemm_32x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us     189.000us         0.43%     189.000us       9.450us           0 b           0 b           0 b           0 b            20  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      86.000us         0.20%      86.000us       3.071us           0 b           0 b           0 b           0 b            28  \n",
      "                                             aten::add_         0.01%     289.000us         0.02%     438.000us      17.520us      75.000us         0.17%      75.000us       3.000us           0 b           0 b           0 b           0 b            25  \n",
      "                         volta_sgemm_64x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us      64.000us         0.15%      64.000us      16.000us           0 b           0 b           0 b           0 b             4  \n",
      "                                       aten::layer_norm         0.00%      48.000us         0.03%     643.000us      71.444us       0.000us         0.00%      58.000us       6.444us           0 b           0 b     333.00 Kb           0 b             9  \n",
      "                                aten::native_layer_norm         0.01%     269.000us         0.03%     595.000us      66.111us      58.000us         0.13%      58.000us       6.444us           0 b           0 b     333.00 Kb           0 b             9  \n",
      "                                              aten::bmm         0.01%     213.000us         0.02%     356.000us      44.500us      58.000us         0.13%      58.000us       7.250us           0 b           0 b     186.00 Kb           0 b             8  \n",
      "void splitKreduce_kernel<float, float, float, float>...         0.00%       0.000us         0.00%       0.000us       0.000us      50.000us         0.11%      50.000us       2.083us           0 b           0 b           0 b           0 b            24  \n",
      "void at::native::(anonymous namespace)::RowwiseMomen...         0.00%       0.000us         0.00%       0.000us       0.000us      40.000us         0.09%      40.000us       4.444us           0 b           0 b           0 b           0 b             9  \n",
      "                                  volta_sgemm_128x32_tn         0.00%       0.000us         0.00%       0.000us       0.000us      34.000us         0.08%      34.000us       8.500us           0 b           0 b           0 b           0 b             4  \n",
      "                                  volta_sgemm_128x32_nn         0.00%       0.000us         0.00%       0.000us       0.000us      24.000us         0.05%      24.000us       6.000us           0 b           0 b           0 b           0 b             4  \n",
      "                                              aten::add         0.01%     226.000us         0.01%     306.000us      23.538us      23.000us         0.05%      23.000us       1.769us           0 b           0 b     366.00 Kb     366.00 Kb            13  \n",
      "                                       aten::contiguous         0.00%      13.000us         0.01%     181.000us      45.250us       0.000us         0.00%      19.000us       4.750us           0 b           0 b     144.00 Kb           0 b             4  \n",
      "                                            aten::clone         0.00%      40.000us         0.01%     168.000us      42.000us       0.000us         0.00%      19.000us       4.750us           0 b           0 b     144.00 Kb           0 b             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      19.000us         0.04%      19.000us       4.750us           0 b           0 b           0 b           0 b             4  \n",
      "void at::native::(anonymous namespace)::LayerNormFor...         0.00%       0.000us         0.00%       0.000us       0.000us      18.000us         0.04%      18.000us       2.000us           0 b           0 b           0 b           0 b             9  \n",
      "                                            aten::addmm         0.00%      91.000us         0.01%     161.000us      80.500us      15.000us         0.03%      15.000us       7.500us           0 b           0 b       2.50 Kb       2.50 Kb             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      12.000us         0.03%      12.000us       1.200us           0 b           0 b           0 b           0 b            10  \n",
      "                                          aten::softmax         0.00%      18.000us         0.01%     119.000us      29.750us       0.000us         0.00%      10.000us       2.500us           0 b           0 b      42.00 Kb           0 b             4  \n",
      "                                         aten::_softmax         0.00%      72.000us         0.00%     101.000us      25.250us      10.000us         0.02%      10.000us       2.500us           0 b           0 b      42.00 Kb      42.00 Kb             4  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us      10.000us         0.02%      10.000us       2.500us           0 b           0 b           0 b           0 b             4  \n",
      "                                             aten::gelu         0.00%      70.000us         0.00%      99.000us      24.750us      10.000us         0.02%      10.000us       2.500us           0 b           0 b     576.00 Kb     576.00 Kb             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.000us         0.02%      10.000us       2.500us           0 b           0 b           0 b           0 b             4  \n",
      "                                        aten::embedding         0.00%      53.000us         0.01%     213.000us      71.000us       0.000us         0.00%       9.000us       3.000us           0 b           0 b     108.00 Kb           0 b             3  \n",
      "                                     aten::index_select         0.00%      51.000us         0.01%     131.000us      43.667us       9.000us         0.02%       9.000us       3.000us           0 b           0 b     108.00 Kb           0 b             3  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us       9.000us         0.02%       9.000us       3.000us           0 b           0 b           0 b           0 b             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.000us         0.02%       8.000us       1.600us           0 b           0 b           0 b           0 b             5  \n",
      "                                              aten::div         0.00%      82.000us         0.00%     112.000us      28.000us       6.000us         0.01%       6.000us       1.500us           0 b           0 b      42.00 Kb      42.00 Kb             4  \n",
      "void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us         0.01%       6.000us       6.000us           0 b           0 b           0 b           0 b             1  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         0.01%       4.000us       4.000us           0 b           0 b           0 b           0 b             1  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         0.01%       3.000us       1.500us           0 b           0 b           0 b           0 b             2  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         0.01%       3.000us       3.000us           0 b           0 b           0 b           0 b             1  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         0.01%       3.000us       3.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                             aten::rsub         0.00%      34.000us         0.00%      94.000us      94.000us       0.000us         0.00%       2.000us       2.000us           0 b           0 b         512 b           0 b             1  \n",
      "                                              aten::sub         0.00%      46.000us         0.00%      60.000us      60.000us       2.000us         0.00%       2.000us       2.000us           0 b           0 b         512 b         512 b             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         0.00%       2.000us       2.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                              aten::mul         0.00%      19.000us         0.00%      27.000us      27.000us       2.000us         0.00%       2.000us       2.000us           0 b           0 b         512 b         512 b             1  \n",
      "                                             aten::tanh         0.00%      21.000us         0.00%      29.000us      29.000us       2.000us         0.00%       2.000us       2.000us           0 b           0 b       2.00 Kb       2.00 Kb             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         0.00%       2.000us       2.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                            aten::zeros         0.35%       8.333ms         0.45%      10.566ms       5.283ms       0.000us         0.00%       0.000us       0.000us       4.00 Kb           0 b           0 b           0 b             2  \n",
      "                                            aten::empty         0.39%       9.204ms         0.39%       9.204ms      46.959us       0.000us         0.00%       0.000us       0.000us     109.73 Mb     109.73 Mb     663.00 Kb     663.00 Kb           196  \n",
      "                                            aten::zero_         0.00%      25.000us         0.00%      25.000us       2.273us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            11  \n",
      "                                          aten::detach_         0.02%     382.000us         0.02%     491.000us       6.377us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            77  \n",
      "                                                detach_         0.00%     109.000us         0.00%     109.000us       1.416us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            77  \n",
      "                                             aten::set_         0.01%     235.000us         0.01%     235.000us       3.176us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            74  \n",
      "                                           aten::detach         0.02%     494.000us         0.04%     852.000us       5.796us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           147  \n",
      "                                                 detach         0.02%     358.000us         0.02%     358.000us       2.435us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           147  \n",
      "                                          aten::normal_         5.38%     126.852ms         5.38%     126.852ms      42.284ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             3  \n",
      "                                           aten::select         0.00%      26.000us         0.00%      30.000us      15.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             2  \n",
      "                                       aten::as_strided         0.00%     101.000us         0.00%     101.000us       1.312us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            77  \n",
      "                                            aten::fill_         0.00%      57.000us         0.00%      57.000us       5.700us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            10  \n",
      "                                           aten::arange         0.00%      28.000us         0.00%      63.000us      31.500us       0.000us         0.00%       0.000us       0.000us       8.00 Kb           0 b           0 b           0 b             2  \n",
      "                                          aten::resize_         0.00%      40.000us         0.00%      40.000us      10.000us       0.000us         0.00%       0.000us       0.000us       4.00 Kb       4.00 Kb     108.00 Kb     108.00 Kb             4  \n",
      "                                           aten::expand         0.01%     127.000us         0.01%     155.000us       8.158us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            19  \n",
      "                                         aten::uniform_         3.67%      86.558ms         3.67%      86.558ms       1.665ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            52  \n",
      "                                    aten::empty_strided         0.16%       3.770ms         0.16%       3.770ms      47.722us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     109.74 Mb     109.74 Mb            79  \n",
      "                                        cudaMemcpyAsync         2.18%      51.436ms         2.18%      51.436ms     642.950us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            80  \n",
      "                                  cudaStreamSynchronize         0.80%      18.964ms         0.80%      18.964ms     243.128us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            78  \n",
      "                aten::_has_compatible_shallow_copy_type         0.01%     124.000us         0.01%     124.000us       0.849us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           146  \n",
      "                                        aten::unsqueeze         0.00%      52.000us         0.00%      61.000us      12.200us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                                            aten::slice         0.00%      56.000us         0.00%      63.000us      12.600us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                                       cudaLaunchKernel        78.95%        1.860s        78.95%        1.860s      13.481ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           138  \n",
      "                                          aten::reshape         0.00%      84.000us         0.01%     145.000us       7.632us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            19  \n",
      "                                   aten::_reshape_alias         0.00%      61.000us         0.00%      61.000us       3.211us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            19  \n",
      "                                             aten::view         0.01%     200.000us         0.01%     200.000us       2.151us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            93  \n",
      "                                          aten::dropout         0.00%       7.000us         0.00%       7.000us       0.500us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            14  \n",
      "                                                aten::t         0.01%     199.000us         0.04%       1.029ms      39.577us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            26  \n",
      "                                        aten::transpose         0.04%     826.000us         0.04%     860.000us      28.667us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            30  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.00%      32.000us         0.00%      32.000us       0.432us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            74  \n",
      "                                         cudaEventQuery         0.00%      51.000us         0.00%      51.000us       2.040us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            25  \n",
      "                                        cudaEventRecord         0.00%      26.000us         0.00%      26.000us       1.040us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            25  \n",
      "                                     aten::_unsafe_view         0.01%     195.000us         0.01%     237.000us       7.406us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            32  \n",
      "                                          aten::permute         0.00%     104.000us         0.01%     123.000us       7.688us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            16  \n",
      "                                       aten::empty_like         0.00%      18.000us         0.00%      52.000us      13.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     144.00 Kb           0 b             4  \n",
      "                                  cudaDeviceSynchronize         0.00%       6.000us         0.00%       6.000us       6.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.356s\n",
      "Self CUDA time total: 43.683ms\n",
      "\n",
      "time per sample = 0.0006852763701500263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/vocab.txt from cache at /home/pfernand/.cache/huggingface/transformers/68be80309844e53b628e9d479926a991d0adf337752bb941f0188887240313b8.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /home/pfernand/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /home/pfernand/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file bert-small-ft-mnli/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert-small-ft-mnli/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at bert-small-ft-mnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1227' max='1227' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1227/1227 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         5.12%     120.392ms        99.99%        2.353s        2.353s       0.000us         0.00%      29.921ms      29.921ms     109.73 Mb      -4.37 Kb     110.13 Mb      -2.51 Mb             1  \n",
      "                                            aten::copy_         0.60%      14.090ms        85.01%        2.001s      12.745ms      29.426ms        98.35%      29.426ms     187.427us           0 b           0 b           0 b           0 b           157  \n",
      "                                               aten::to         0.07%       1.603ms        84.75%        1.995s      12.787ms       0.000us         0.00%      29.409ms     188.519us           0 b           0 b     110.13 Mb           0 b           156  \n",
      "                                         aten::_to_copy         0.09%       2.126ms        84.68%        1.993s      25.229ms       0.000us         0.00%      29.409ms     372.266us           0 b           0 b     110.13 Mb           0 b            79  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      29.406ms        98.28%      29.406ms     377.000us           0 b           0 b           0 b           0 b            78  \n",
      "                                           aten::linear         0.01%     201.000us         0.14%       3.211ms     123.500us       0.000us         0.00%     356.000us      13.692us           0 b           0 b       1.06 Mb           0 b            26  \n",
      "                                           aten::matmul         0.02%     440.000us         0.09%       2.228ms      69.625us       0.000us         0.00%     309.000us       9.656us           0 b           0 b       1.20 Mb           0 b            32  \n",
      "                                               aten::mm         0.03%     679.000us         0.04%     936.000us      39.000us     288.000us         0.96%     288.000us      12.000us           0 b           0 b       1.05 Mb       1.05 Mb            24  \n",
      "void gemmSN_TN_kernel_64addr<float, 128, 16, 2, 4, 8...         0.00%       0.000us         0.00%       0.000us       0.000us     147.000us         0.49%     147.000us       7.350us           0 b           0 b           0 b           0 b            20  \n",
      "                         volta_sgemm_32x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us      80.000us         0.27%      80.000us      20.000us           0 b           0 b           0 b           0 b             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      64.000us         0.21%      64.000us       2.286us           0 b           0 b           0 b           0 b            28  \n",
      "                         volta_sgemm_64x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us      61.000us         0.20%      61.000us      15.250us           0 b           0 b           0 b           0 b             4  \n",
      "                                             aten::add_         0.01%     277.000us         0.02%     443.000us      17.720us      55.000us         0.18%      55.000us       2.200us           0 b           0 b           0 b           0 b            25  \n",
      "                                       aten::layer_norm         0.00%      51.000us         0.03%     652.000us      72.444us       0.000us         0.00%      46.000us       5.111us           0 b           0 b     279.00 Kb           0 b             9  \n",
      "                                aten::native_layer_norm         0.01%     271.000us         0.03%     601.000us      66.778us      46.000us         0.15%      46.000us       5.111us           0 b           0 b     279.00 Kb           0 b             9  \n",
      "void at::native::(anonymous namespace)::RowwiseMomen...         0.00%       0.000us         0.00%       0.000us       0.000us      33.000us         0.11%      33.000us       3.667us           0 b           0 b           0 b           0 b             9  \n",
      "                                        aten::embedding         0.00%      50.000us         0.01%     192.000us      64.000us       0.000us         0.00%      26.000us       8.667us           0 b           0 b      90.00 Kb           0 b             3  \n",
      "                                     aten::index_select         0.00%      53.000us         0.00%     115.000us      38.333us      26.000us         0.09%      26.000us       8.667us           0 b           0 b      90.00 Kb           0 b             3  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us      26.000us         0.09%      26.000us       8.667us           0 b           0 b           0 b           0 b             3  \n",
      "                                              aten::bmm         0.01%     169.000us         0.01%     306.000us      38.250us      21.000us         0.07%      21.000us       2.625us           0 b           0 b     150.00 Kb           0 b             8  \n",
      "                                              aten::add         0.01%     222.000us         0.01%     302.000us      23.231us      18.000us         0.06%      18.000us       1.385us           0 b           0 b     300.00 Kb     300.00 Kb            13  \n",
      "                                       aten::contiguous         0.00%      15.000us         0.01%     181.000us      45.250us       0.000us         0.00%      17.000us       4.250us           0 b           0 b     120.00 Kb           0 b             4  \n",
      "                                            aten::clone         0.00%      38.000us         0.01%     166.000us      41.500us       0.000us         0.00%      17.000us       4.250us           0 b           0 b     120.00 Kb           0 b             4  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      17.000us         0.06%      17.000us       4.250us           0 b           0 b           0 b           0 b             4  \n",
      "                                            aten::addmm         0.00%      80.000us         0.01%     153.000us      76.500us      14.000us         0.05%      14.000us       7.000us           0 b           0 b       2.50 Kb       2.50 Kb             2  \n",
      "void at::native::(anonymous namespace)::LayerNormFor...         0.00%       0.000us         0.00%       0.000us       0.000us      13.000us         0.04%      13.000us       1.444us           0 b           0 b           0 b           0 b             9  \n",
      "void gemmSN_NN_kernel<float, 256, 4, 2, 8, 4, 4, fal...         0.00%       0.000us         0.00%       0.000us       0.000us      12.000us         0.04%      12.000us       3.000us           0 b           0 b           0 b           0 b             4  \n",
      "                                             aten::gelu         0.00%      69.000us         0.00%      97.000us      24.250us      11.000us         0.04%      11.000us       2.750us           0 b           0 b     480.00 Kb     480.00 Kb             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      11.000us         0.04%      11.000us       2.750us           0 b           0 b           0 b           0 b             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       9.000us         0.03%       9.000us       0.900us           0 b           0 b           0 b           0 b            10  \n",
      "void splitKreduce_kernel<float, float, float, float>...         0.00%       0.000us         0.00%       0.000us       0.000us       9.000us         0.03%       9.000us       2.250us           0 b           0 b           0 b           0 b             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.000us         0.02%       7.000us       1.400us           0 b           0 b           0 b           0 b             5  \n",
      "                                          aten::softmax         0.00%      17.000us         0.00%     117.000us      29.250us       0.000us         0.00%       6.000us       1.500us           0 b           0 b      30.00 Kb           0 b             4  \n",
      "                                         aten::_softmax         0.00%      73.000us         0.00%     100.000us      25.000us       6.000us         0.02%       6.000us       1.500us           0 b           0 b      30.00 Kb      30.00 Kb             4  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us         0.02%       6.000us       1.500us           0 b           0 b           0 b           0 b             4  \n",
      "                                              aten::div         0.00%      81.000us         0.00%     111.000us      27.750us       5.000us         0.02%       5.000us       1.250us           0 b           0 b      30.00 Kb      30.00 Kb             4  \n",
      "void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us       5.000us         0.02%       5.000us       5.000us           0 b           0 b           0 b           0 b             1  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         0.01%       3.000us       3.000us           0 b           0 b           0 b           0 b             1  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         0.01%       3.000us       1.500us           0 b           0 b           0 b           0 b             2  \n",
      "void dot_kernel<float, 128, 0, cublasDotParams<cubla...         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         0.01%       3.000us       3.000us           0 b           0 b           0 b           0 b             1  \n",
      "void reduce_1Block_kernel<float, 128, 7, cublasGemvT...         0.00%       0.000us         0.00%       0.000us       0.000us       3.000us         0.01%       3.000us       3.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                              aten::mul         0.00%      18.000us         0.00%      26.000us      26.000us       2.000us         0.01%       2.000us       2.000us           0 b           0 b         512 b         512 b             1  \n",
      "                                             aten::tanh         0.00%      61.000us         0.00%      70.000us      70.000us       2.000us         0.01%       2.000us       2.000us           0 b           0 b       2.00 Kb       2.00 Kb             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         0.01%       2.000us       2.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                             aten::rsub         0.00%      34.000us         0.00%      92.000us      92.000us       0.000us         0.00%       1.000us       1.000us           0 b           0 b         512 b           0 b             1  \n",
      "                                              aten::sub         0.00%      44.000us         0.00%      58.000us      58.000us       1.000us         0.00%       1.000us       1.000us           0 b           0 b         512 b         512 b             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         0.00%       1.000us       1.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                            aten::zeros         0.01%     156.000us         0.01%     207.000us     103.500us       0.000us         0.00%       0.000us       0.000us       4.00 Kb           0 b           0 b           0 b             2  \n",
      "                                            aten::empty         0.26%       6.113ms         0.26%       6.113ms      31.189us       0.000us         0.00%       0.000us       0.000us     109.73 Mb     109.73 Mb     549.00 Kb     549.00 Kb           196  \n",
      "                                            aten::zero_         0.00%      27.000us         0.00%      27.000us       2.455us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            11  \n",
      "                                          aten::detach_         0.02%     398.000us         0.02%     519.000us       6.740us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            77  \n",
      "                                                detach_         0.01%     121.000us         0.01%     121.000us       1.571us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            77  \n",
      "                                             aten::set_         0.01%     245.000us         0.01%     245.000us       3.311us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            74  \n",
      "                                           aten::detach         0.02%     538.000us         0.04%     924.000us       6.286us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           147  \n",
      "                                                 detach         0.02%     386.000us         0.02%     386.000us       2.626us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           147  \n",
      "                                          aten::normal_         5.38%     126.687ms         5.38%     126.687ms      42.229ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             3  \n",
      "                                           aten::select         0.00%      27.000us         0.00%      30.000us      15.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             2  \n",
      "                                       aten::as_strided         0.00%      95.000us         0.00%      95.000us       1.234us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            77  \n",
      "                                            aten::fill_         0.00%      56.000us         0.00%      56.000us       5.600us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            10  \n",
      "                                           aten::arange         0.00%      33.000us         0.00%      72.000us      36.000us       0.000us         0.00%       0.000us       0.000us       8.00 Kb           0 b           0 b           0 b             2  \n",
      "                                          aten::resize_         0.00%      40.000us         0.00%      40.000us      10.000us       0.000us         0.00%       0.000us       0.000us       4.00 Kb       4.00 Kb      90.00 Kb      90.00 Kb             4  \n",
      "                                           aten::expand         0.01%     137.000us         0.01%     154.000us       8.105us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            19  \n",
      "                                         aten::uniform_         3.61%      84.968ms         3.61%      84.968ms       1.634ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            52  \n",
      "                                    aten::empty_strided         0.12%       2.828ms         0.12%       2.828ms      35.797us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     110.13 Mb     110.13 Mb            79  \n",
      "                                        cudaMemcpyAsync         1.58%      37.074ms         1.58%      37.074ms     463.425us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            80  \n",
      "                                  cudaStreamSynchronize         0.12%       2.868ms         0.12%       2.868ms      36.769us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            78  \n",
      "                aten::_has_compatible_shallow_copy_type         0.01%     180.000us         0.01%     180.000us       1.233us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           146  \n",
      "                                        aten::unsqueeze         0.00%      77.000us         0.00%      90.000us      18.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                                            aten::slice         0.00%      59.000us         0.00%      69.000us      13.800us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                                       cudaLaunchKernel        82.75%        1.948s        82.75%        1.948s      16.506ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           118  \n",
      "                                          aten::reshape         0.00%      85.000us         0.01%     140.000us       7.368us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            19  \n",
      "                                   aten::_reshape_alias         0.00%      55.000us         0.00%      55.000us       2.895us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            19  \n",
      "                                             aten::view         0.01%     193.000us         0.01%     193.000us       2.075us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            93  \n",
      "                                          aten::dropout         0.00%      12.000us         0.00%      12.000us       0.857us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            14  \n",
      "                                                aten::t         0.01%     194.000us         0.04%     960.000us      36.923us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            26  \n",
      "                                        aten::transpose         0.03%     758.000us         0.03%     794.000us      26.467us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            30  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.00%      22.000us         0.00%      22.000us       0.407us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            54  \n",
      "                                     aten::_unsafe_view         0.01%     181.000us         0.01%     222.000us       6.938us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            32  \n",
      "                                          aten::permute         0.00%     111.000us         0.01%     127.000us       7.938us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            16  \n",
      "                                       aten::empty_like         0.00%      20.000us         0.00%      53.000us      13.250us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     120.00 Kb           0 b             4  \n",
      "                                         cudaEventQuery         0.00%      16.000us         0.00%      16.000us       3.200us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                                        cudaEventRecord         0.00%       6.000us         0.00%       6.000us       1.200us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                                  cudaDeviceSynchronize         0.00%       6.000us         0.00%       6.000us       6.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.354s\n",
      "Self CUDA time total: 29.921ms\n",
      "\n",
      "time per sample = 0.0007633578022766405\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from timeit import default_timer as timer\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "for task in TASKS:\n",
    "    finetuned_path = f\"{PRETRAINED_MODEL.split('/')[-1]}-ft-{task}\"\n",
    "    #model = BertForSequenceClassification.from_pretrained(finetuned_path)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(PRETRAINED_MODEL)\n",
    "    val_key = \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CUDA, ProfilerActivity.CPU], profile_memory=True, record_shapes=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            inputs = tokenized_datasets[task][val_key][0]\n",
    "            model = BertForSequenceClassification.from_pretrained(finetuned_path).cuda()\n",
    "            model.forward(\n",
    "                input_ids=torch.tensor(inputs[\"input_ids\"]).unsqueeze(0).cuda(), \n",
    "                token_type_ids=torch.tensor(inputs[\"token_type_ids\"]).unsqueeze(0).cuda(), \n",
    "                attention_mask=torch.tensor(inputs[\"attention_mask\"]).unsqueeze(0).cuda()\n",
    "            )\n",
    "    start = timer()\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir = finetuned_path,\n",
    "        do_train = False,\n",
    "        do_predict = True,\n",
    "        dataloader_drop_last = False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        eval_args,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, task=task)\n",
    "    )\n",
    "    trainer.predict(tokenized_datasets[task][val_key])\n",
    "    end = timer()\n",
    "    delta = end-start\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n",
    "    print(f\"time per sample = {delta/len(tokenized_datasets[task][val_key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e25a689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'sentence': 'The sailors rode the breeze clear of the rocks.', 'idx': 0, 'label': 1, 'input_ids': [101, 1996, 11279, 8469, 1996, 9478, 3154, 1997, 1996, 5749, 1012, 102]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[task][val_key][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ac1a4",
   "metadata": {},
   "source": [
    "## Divide Checkpoint\n",
    "\n",
    "Code for dividing a checkpoint by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51be41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ft-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09acde2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkpoint_divider import divide_checkpoint\n",
    "\n",
    "divide_checkpoint(\"ft-bert-base-uncased/pytorch_model.bin\", divided_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff4f0d",
   "metadata": {},
   "source": [
    "## Load Divided Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30097e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_memory():\n",
    "    import os, psutil\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a051afcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {0: ['bert.embeddings.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.position_ids', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.word_embeddings.weight'], 1: ['bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight'], 2: ['bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight'], 3: ['bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight'], 4: ['bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight'], 5: ['bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight'], 6: ['bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight'], 7: ['bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight'], 8: ['bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight'], 9: ['bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight'], 10: ['bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight'], 11: ['bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight'], 12: ['bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight'], 13: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.load(f\"{divided_checkpoint}/layer2keys.bin\"))\n",
    "layers=[]\n",
    "memory=[]\n",
    "for layer in range(14):\n",
    "    layers.append(torch.load(f\"{divided_checkpoint}/pytorch_model_{layer}.bin\"))\n",
    "    memory.append(get_current_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a52e3052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe8797ca810>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34212c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:11-767] *",
   "language": "python",
   "name": "conda-env-11-767-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
